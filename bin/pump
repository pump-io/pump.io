#!/usr/bin/env node

// -*- mode: javascript -*-

// pump.io
//
// entry point activity pump application
//
// Copyright 2011-2012, E14N https://e14n.com/
// Copyright 2017-2018 AJ Jordan <alex@strugee.net>
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

"use strict";

var cluster = require("cluster"),
    os = require("os"),
    fs = require("fs"),
    path = require("path"),
    net = require("net"),
    Logger = require("bunyan"),
    _ = require("lodash"),
    split = require("split"),
    onExit = require("signal-exit"),
    configUtil = require("../lib/config"),
    restartUtil = require("../lib/restartutil"),
    restartInflight = false,
    abortedRestart = false,
    restartedWorker = null,
    argv = require("yargs")
           .usage("Usage: $0 -c <configfile>")
           .alias("c", "config")
           // Hack to make sure that .config()'s callback gets run even if -c isn't specified
           .default("c", "/dev/null", "/etc/pump.io.json and ~/.pump.io.json")
           .config("c", "Configuration file", function findConfig(filename) {
               var files,
                   config = {},
                   i,
                   raw,
                   parsed;

               if (filename !== "/dev/null") {
                   files = [filename];
               } else {
                   files = ["/etc/pump.io.json"];
                   if (process.env.HOME) {
                       files.push(path.join(process.env.HOME, ".pump.io.json"));
                   }
               }

               // This is all sync
               for (i = 0; i < files.length; i++) {
                   try {
                       raw = fs.readFileSync(files[i]);
                   } catch (err) {
                       continue;
                   }

                   try {
                       parsed = JSON.parse(raw);
                       _.extend(config, parsed);
                   } catch (err) {
                       console.error("Error parsing JSON configuration:", err.toString());
                       console.error("Try using a JSON validator.");
                       process.exit(1);
                   }
               }

               return config;
           })
           .env("PUMPIO")
           // TODO more validation of booleans, etc. here
           .number(["port", "urlPort", "smtpport", "smtptimeout", "cleanupSession", "cleanupNonce"])
           .string(["driver", "hostname", "address", "secret", "site", "owner", "ownerURL", "mainImage", "appendFooter", "logfile", "logLevel", "serverUser", "key", "cert", "datadir", "firehose", "spamhost", "spamclientid", "spamclientsecret", "smtpserver", "smtpuser", "smtppass", "smtpfrom", "favicon"])
           .boolean(["bounce", "noweb", "nologger", "enableUploads", "debugClient", "disableRegistration", "noCDN", "requireEmail", "smtpusetls", "smtpusessl", "compress", "sockjs"])
           .alias("help", "h")
           .help()
           .alias("version", "v")
           .version()
           .argv,
    Dispatch = require("../lib/dispatch"),
    makeApp = require("../lib/app").makeApp;

// We first get config files and launch some cluster apps

var main = function() {
    var config = argv;
    launchApps(config);
};

// Launch apps

var launchApps = function(configBase) {
    var config = configUtil.buildConfig(configBase),
        i,
        log = setupLog(config),
        clusterLog = log.child({component: "cluster"});

    if (cluster.isMaster) {

        // We pass configBase because we want what the user originally gave
        // The only reason we waited this long (and until after parsing) was to get a log
        configUtil.validateConfig(configBase, log);

        Dispatch.start(log);

        for (i = 0; i < config.workers; i++) {
            cluster.fork();
        }

        // Restart child processes when they die

        cluster.on("exit", function(worker, code, signal) {
            var zeroDowntimeRestart = worker.exitedAfterDisconnect,
                exitCode = worker.process.exitCode;

            // Don't respawn any workers if we aborted a restart
            // Note that even for crashes it doesn't make sense to respawn since we _know_ all new workers will be bad
            if (abortedRestart) {
                clusterLog.error("worker " + worker.process.pid + " died ("+exitCode+") but refusing to restart due to aborted zero-downtime restart; please restart this process");
                return;
            }

            // Check if a zero-downtime worker respawn went bad
            if (worker.process.pid === restartedWorker) {
                clusterLog.error("worker " + worker.process.pid + " died ("+exitCode+") directly after respawn for zero-downtime restart");
                abortedRestart = true;
                restartedWorker = null;
            }

            if (!zeroDowntimeRestart) {
                clusterLog.error("worker " + worker.process.pid + " died ("+exitCode+"). restarting...");
            }

            cluster.fork();
        });

        var ctl = net.createServer(function(con) {
            con.pipe(split()).on("data", function(line) {
                switch (line.toLowerCase()) {
                case "restart":
                    clusterLog.info("Received RESTART command over control socket.");
                    var restartCheck = restartUtil.checkRequirements(config, abortedRestart, restartInflight);
                    if (!restartCheck.ok) {
                        con.write("NACK ");
                        switch (restartCheck.reason) {
                        case "no workers":
                            log.warn("Received restart request but ignoring because there aren't enough workers for zero-downtime restart");
                            con.write("no workers");
                            break;
                        case "bad driver":
                            log.warn("Zero-downtime restarts are only supported on MongoDB for the time being");
                            con.write("bad driver");
                            break;
                        case "aborted restart":
                            log.warn("Received restart request but refusing due to an earlier aborted restart; please restart this process");
                            con.write("aborted restart");
                            break;
                        case "restart inflight":
                            log.warn("Received restart request but ignoring because there's already an in-flight zero-downtime restart");
                            con.write("restart in flight");
                            break;
                        case "bad epoch":
                            log.warn("Refusing zero-downtime restart because the new version says it's incompatible with us");
                            con.write("incompatibility");
                            break;
                        default:
                            throw new Error("Unknown failure reason " + restartCheck.reason);
                        }
                        con.write("\n");
                        return;
                    }

                    triggerRestart(clusterLog, config);
                    con.write("ACK\n");
                    break;
                default:
                    clusterLog.info("Received unknown command over control socket; rejecting.", {socketCmd: line});
                    con.write("NACK unknown\n");
                }
            });
        });

        ctl.listen(config.controlSocket, function() {
            clusterLog.info("Listening for control commands on path " + config.controlSocket + ".");
        });

        onExit(function() {
            try {
                fs.unlinkSync(config.controlSocket);
            } catch (e) {
                if (e.code !== "ENOENT") throw e;
            }
        });

    } else {

        makeApp(configBase, function(err, appServer) {
            if (err) {
                console.error(err);
                process.exit(1);
            } else {
                appServer.run(function(err) {
                    if (err) {
                        console.error(err);
                        process.exit(1);
                    }
                });
            }
        });
    }
};

// Initialize a zero-downtime restart

var triggerRestart = function(clusterLog, config) {
    restartInflight = true;

    clusterLog.info("Received SIGUSR2; starting zero-downtime restart...");

    clusterLog.debug("Determining zero-downtime restart eligibility...");

    var reqs = restartUtil.checkRequirements(config, abortedRestart, restartInflight);
    if (!reqs.ok) return;

    var workers = [];
    for (var id in cluster.workers) {
        workers.push(cluster.workers[id]);
    }

    restartWorker(workers.pop(), workers, clusterLog, function() {
        clusterLog.info("Zero-downtime restart complete");
        restartInflight = false;
    });
};

// Restart a worker and schedule the next restart

var restartWorker = function(worker, workerQueue, log, cb) {
    worker.send({cmd: "gracefulShutdown"});
    log.debug("Killed " + worker.process.pid + " for zero-downtime restart.");

    var timeout = setTimeout(function() {
        log.warn("Killed " + worker.process.pid + " because it didn't shut down within 30 seconds.");
        worker.kill();
    }, 30 * 1000);

    worker.once("exit", function() {
        clearTimeout(timeout);
    });

    // TODO error handling?
    cluster.once("fork", function(worker) {
        log.debug("Respawned " + worker.process.pid + " with new code.");
        restartedWorker = worker.process.pid;

        // We don't use the 'listening' event and wait for
        // explicit notification because 'listen' would fire when
        // the main server was listening but not the bounce server
        worker.once("message", function(obj) {
            switch (obj.msg) {
            case "listening":
                if (workerQueue.length === 0) {
                    cb();
                    return;
                }

                restartWorker(workerQueue.pop(), workerQueue, log, cb);

                restartedWorker = null;
                break;
            case "abortRestart":
                log.warn(obj.err, "New worker signaled error; the master is in an inconsistent state and should be restarted ASAP");
                abortedRestart = true;
                break;
            default:
                // msg is an internal Bunyan field so we rename it
                obj.workerMsg = obj.msg;
                delete obj.msg;
                log.error(obj, "Received unknown restart status message from worker");
                throw new Error("Received unknown restart status message from worker: " + obj.workerMsg);
            }
        });
    });
};

// Set up a Bunyan logger that the dispatch code can send to

var setupLog = function(config) {

    var log,
        logParams = {
            name: "pump.io"
        };

    if (config.logfile) {
        logParams.streams = [{path: config.logfile}];
    } else if (config.nologger) {
        logParams.streams = [{path: "/dev/null"}];
    } else {
        logParams.streams = [{stream: process.stderr}];
    }

    logParams.streams[0].level = config.logLevel;

    return new Logger(logParams);
};

main();
